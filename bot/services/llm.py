"""
MysticBot ‚Äî LLM Service
–ï–¥–∏–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏.
–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: Featherless ‚Üí Perplexity ‚Üí OpenAI.
–ü–æ–¥–¥–µ—Ä–∂–∫–∞: retry –ø—Ä–∏ 503, —Ç–∞–π–º–∞—É—Ç—ã, graceful fallback.
"""

import asyncio
import logging
import time
from typing import Optional

import httpx

from bot.config import settings, FeatherlessConfig, PerplexityConfig, OpenAIConfig

logger = logging.getLogger(__name__)


class LLMError(Exception):
    """–ë–∞–∑–æ–≤–∞—è –æ—à–∏–±–∫–∞ LLM-—Å–µ—Ä–≤–∏—Å–∞."""
    pass


class AllProvidersFailedError(LLMError):
    """–í—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."""
    pass


class LLMProvider:
    """
    –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ LLM-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API).
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç retry –ø—Ä–∏ 503 –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π —Ç–∞–π–º–∞—É—Ç.
    """

    def __init__(
        self,
        name: str,
        api_key: str,
        base_url: str,
        model: str,
        timeout: int = 60,
        max_retries: int = 3,
        retry_delay: float = 30.0,
    ):
        self.name = name
        self.api_key = api_key
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.timeout = timeout
        self.max_retries = max_retries
        self.retry_delay = retry_delay

        # –ï–¥–∏–Ω—ã–π httpx-–∫–ª–∏–µ–Ω—Ç —Å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º
        self._client: Optional[httpx.AsyncClient] = None

    async def _get_client(self) -> httpx.AsyncClient:
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è httpx-–∫–ª–∏–µ–Ω—Ç–∞."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                base_url=self.base_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
                timeout=httpx.Timeout(
                    connect=10.0,
                    read=float(self.timeout),
                    write=10.0,
                    pool=10.0,
                ),
            )
        return self._client

    async def chat_completion(
        self,
        messages: list[dict],
        temperature: float = 0.7,
        max_tokens: int = 2048,
        **kwargs,
    ) -> dict:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ /chat/completions —Å retry-–ª–æ–≥–∏–∫–æ–π.

        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π [{role, content}]
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
            **kwargs: –î–æ–ø. –ø–∞—Ä–∞–º–µ—Ç—Ä—ã API

        Returns:
            dict —Å –æ—Ç–≤–µ—Ç–æ–º API

        Raises:
            LLMError: –ø—Ä–∏ –Ω–µ—É—Å–ø–µ—à–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ –ø–æ—Å–ª–µ –≤—Å–µ—Ö retry
        """
        client = await self._get_client()

        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            **kwargs,
        }

        last_error: Optional[Exception] = None

        for attempt in range(1, self.max_retries + 1):
            start_time = time.monotonic()
            try:
                response = await client.post(
                    "/chat/completions",
                    json=payload,
                )

                elapsed = time.monotonic() - start_time

                # –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç
                if response.status_code == 200:
                    data = response.json()
                    logger.info(
                        f"‚úÖ [{self.name}] –æ—Ç–≤–µ—Ç –∑–∞ {elapsed:.1f}—Å "
                        f"(–º–æ–¥–µ–ª—å={self.model}, –ø–æ–ø—ã—Ç–∫–∞={attempt})"
                    )
                    return data

                # 503 ‚Äî –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è (Featherless cold start)
                if response.status_code == 503:
                    logger.warning(
                        f"‚è≥ [{self.name}] 503 ‚Äî –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è "
                        f"(–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{self.max_retries}, "
                        f"–æ–∂–∏–¥–∞–Ω–∏–µ {self.retry_delay}—Å)"
                    )
                    if attempt < self.max_retries:
                        await asyncio.sleep(self.retry_delay)
                        continue
                    raise LLMError(
                        f"[{self.name}] 503 –ø–æ—Å–ª–µ {self.max_retries} –ø–æ–ø—ã—Ç–æ–∫ ‚Äî "
                        f"–º–æ–¥–µ–ª—å {self.model} –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å"
                    )

                # 429 ‚Äî rate limit
                if response.status_code == 429:
                    retry_after = float(
                        response.headers.get("Retry-After", "5")
                    )
                    logger.warning(
                        f"üö´ [{self.name}] 429 Rate Limit ‚Äî "
                        f"–æ–∂–∏–¥–∞–Ω–∏–µ {retry_after}—Å"
                    )
                    if attempt < self.max_retries:
                        await asyncio.sleep(retry_after)
                        continue
                    raise LLMError(
                        f"[{self.name}] Rate limit –ø–æ—Å–ª–µ {self.max_retries} –ø–æ–ø—ã—Ç–æ–∫"
                    )

                # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ ‚Äî –Ω–µ —Ä–µ—Ç—Ä–∞–∏–º
                error_body = response.text[:500]
                raise LLMError(
                    f"[{self.name}] HTTP {response.status_code}: {error_body}"
                )

            except httpx.TimeoutException as e:
                elapsed = time.monotonic() - start_time
                logger.warning(
                    f"‚è±Ô∏è [{self.name}] –¢–∞–π–º–∞—É—Ç {elapsed:.1f}—Å "
                    f"(–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{self.max_retries}): {e}"
                )
                last_error = e
                if attempt < self.max_retries:
                    await asyncio.sleep(5)
                    continue

            except httpx.NetworkError as e:
                logger.error(f"üîå [{self.name}] –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞: {e}")
                last_error = e
                if attempt < self.max_retries:
                    await asyncio.sleep(5)
                    continue

        raise LLMError(
            f"[{self.name}] –í—Å–µ {self.max_retries} –ø–æ–ø—ã—Ç–æ–∫ –∏—Å—á–µ—Ä–ø–∞–Ω—ã: {last_error}"
        )

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ httpx-–∫–ª–∏–µ–Ω—Ç–∞."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            logger.debug(f"[{self.name}] httpx-–∫–ª–∏–µ–Ω—Ç –∑–∞–∫—Ä—ã—Ç")


class LLMService:
    """
    –ú—É–ª—å—Ç–∏–ø—Ä–æ–≤–∞–π–¥–µ—Ä–Ω—ã–π LLM-—Å–µ—Ä–≤–∏—Å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback.

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        llm = get_llm_service()
        result = await llm.chat("–ü—Ä–∏–≤–µ—Ç, —Ä–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –¢–∞—Ä–æ")
    """

    def __init__(self):
        self.providers: list[LLMProvider] = []
        self._init_providers()

    def _init_providers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞."""

        # 1. Featherless AI (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        if settings.featherless.enabled:
            self.providers.append(LLMProvider(
                name="Featherless",
                api_key=settings.featherless.api_key,
                base_url=settings.featherless.base_url,
                model=settings.featherless.model,
                timeout=settings.featherless.timeout,         # 180—Å
                max_retries=settings.featherless.max_retries,  # 3
                retry_delay=30.0,
            ))
            logger.info(
                f"ü™∂ Featherless: –º–æ–¥–µ–ª—å={settings.featherless.model}, "
                f"—Ç–∞–π–º–∞—É—Ç={settings.featherless.timeout}—Å"
            )

        # 2. Perplexity (—Ñ–æ–ª–ª–±—ç–∫ #1)
        if settings.perplexity.enabled:
            self.providers.append(LLMProvider(
                name="Perplexity",
                api_key=settings.perplexity.api_key,
                base_url=settings.perplexity.base_url,
                model=settings.perplexity.model,
                timeout=settings.perplexity.timeout,
                max_retries=2,
                retry_delay=5.0,
            ))
            logger.info(f"üîç Perplexity: –º–æ–¥–µ–ª—å={settings.perplexity.model}")

        # 3. OpenAI (—Ñ–æ–ª–ª–±—ç–∫ #2)
        if settings.openai.enabled:
            self.providers.append(LLMProvider(
                name="OpenAI",
                api_key=settings.openai.api_key,
                base_url=settings.openai.base_url,
                model=settings.openai.model,
                timeout=settings.openai.timeout,
                max_retries=2,
                retry_delay=5.0,
            ))
            logger.info(f"ü§ñ OpenAI: –º–æ–¥–µ–ª—å={settings.openai.model}")

        if not self.providers:
            logger.error("‚ùå –ù–∏ –æ–¥–∏–Ω LLM-–ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω!")

    async def chat_completion(
        self,
        messages: list[dict],
        temperature: float = 0.7,
        max_tokens: int = 2048,
        preferred_provider: Optional[str] = None,
        **kwargs,
    ) -> dict:
        """
        –ó–∞–ø—Ä–æ—Å –∫ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback.

        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            max_tokens: –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
            preferred_provider: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–ø–æ –∏–º–µ–Ω–∏)
            **kwargs: –î–æ–ø. –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

        Returns:
            dict ‚Äî –æ—Ç–≤–µ—Ç API

        Raises:
            AllProvidersFailedError: –µ—Å–ª–∏ –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã —É–ø–∞–ª–∏
        """
        errors: list[str] = []

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: preferred_provider –ø–µ—Ä–≤—ã–º
        providers = self.providers
        if preferred_provider:
            providers = sorted(
                self.providers,
                key=lambda p: p.name.lower() != preferred_provider.lower(),
            )

        for provider in providers:
            try:
                return await provider.chat_completion(
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    **kwargs,
                )
            except LLMError as e:
                error_msg = str(e)
                errors.append(error_msg)
                logger.warning(
                    f"‚ö†Ô∏è [{provider.name}] –Ω–µ —É–¥–∞–ª–æ—Å—å ‚Äî "
                    f"–ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {error_msg}"
                )
                continue

        raise AllProvidersFailedError(
            f"–í—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã:\n" +
            "\n".join(f"  ‚Ä¢ {e}" for e in errors)
        )

    async def chat(
        self,
        user_message: str,
        system_prompt: str = "–¢—ã ‚Äî –º–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫ MysticBot.",
        temperature: float = 0.7,
        max_tokens: int = 2048,
        **kwargs,
    ) -> str:
        """
        –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –º–µ—Ç–æ–¥: —Ç–µ–∫—Å—Ç ‚Üí —Ç–µ–∫—Å—Ç.

        Args:
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç

        Returns:
            str ‚Äî —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ]

        data = await self.chat_completion(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs,
        )

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
        try:
            return data["choices"][0]["message"]["content"]
        except (KeyError, IndexError) as e:
            logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: {e}\nData: {data}")
            raise LLMError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ LLM: {e}")

    async def health_check(self) -> dict[str, bool]:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.

        Returns:
            {"Featherless": True, "Perplexity": False, ...}
        """
        results = {}
        for provider in self.providers:
            try:
                await provider.chat_completion(
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=5,
                    temperature=0.0,
                )
                results[provider.name] = True
            except Exception:
                results[provider.name] = False
        return results

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö httpx-–∫–ª–∏–µ–Ω—Ç–æ–≤."""
        for provider in self.providers:
            await provider.close()
        logger.info("üîí –í—Å–µ LLM-–∫–ª–∏–µ–Ω—Ç—ã –∑–∞–∫—Ä—ã—Ç—ã")


# === –°–∏–Ω–≥–ª—Ç–æ–Ω ===
_llm_service: Optional[LLMService] = None


def get_llm_service() -> LLMService:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ/—Å–æ–∑–¥–∞–Ω–∏–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ LLMService."""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service
